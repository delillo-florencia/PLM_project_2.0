blueprint_name: hpc-slurm-kd

vars:
  project_id:  ## Set GCP Project ID Here ##
  deployment_name: hpc-slurm-kd
  region: europe-west4
  zone: europe-west4-a

deployment_groups:
- group: primary
  modules:
  - id: network
    source: modules/network/vpc

  - id: private_service_access
    source: community/modules/network/private-service-access
    use: [network]

  - id: homefs
    source: modules/file-system/filestore
    use: [network, private_service_access]
    settings:
      local_mount: /home
      #capacity_gb: 10240  # 10TB for dataset and checkpoints
      #tier: PREMIUM  # High-performance storage

  - id: compute_nodeset
    source: community/modules/compute/schedmd-slurm-gcp-v6-nodeset
    use: [network]
    settings:
      node_count_dynamic_max: 1 # 8 nodes
      machine_type: a2-ultragpu-4g # 4x NVIDIA A100 GPUs per node
      guest_accelerator:
        - type: nvidia-a100-80gb
          count: 4 # 1 GPU
      disk_type: pd-balanced
      #bandwidth_tier: tier_1_enabled  # High-bandwidth networking
      allow_automatic_updates: false

  - id: compute_partition
    source: community/modules/compute/schedmd-slurm-gcp-v6-partition
    use:
    - compute_nodeset
    settings:
      partition_name: compute
      exclusive: true  # Exclusive access to nodes

  - id: slurm_login
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-login
    use: [network]
    settings:
      machine_type: n2-standard-16  # Larger login node for data preprocessing
      enable_login_public_ips: true

  - id: slurm_controller
    source: community/modules/scheduler/schedmd-slurm-gcp-v6-controller
    use:
    - network
    - compute_partition
    - homefs
    - slurm_login
    settings:
      enable_controller_public_ips: true