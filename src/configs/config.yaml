##define input data sources
#train_csv_file: "/home/dtuteam/workspace/PLM_project_2.0/data/uniref100_500k.csv"
#train_hash_file: "/home/dtuteam/workspace/PLM_project_2.0/data/uniref100_500k_train.hash"
#val_csv_file: "/home/dtuteam/workspace/PLM_project_2.0/data/uniref100_500k.csv"
#val_hash_file: "/home/dtuteam/workspace/PLM_project_2.0/data/uniref100_500k_val.hash"
train_csv_file: "/home/dtuteam/workspace/PLM_project_2.0/data/cluster_uniref100/cluster_uniref100_q2-3_train.csv"
train_hash_file: "/home/dtuteam/workspace/PLM_project_2.0/data/cluster_uniref100/cluster_uniref100_q2-3_train.hash"
val_csv_file: "/home/dtuteam/workspace/PLM_project_2.0/data/cluster_uniref100/cluster_uniref100_q2-3_val.csv"
val_hash_file: "/home/dtuteam/workspace/PLM_project_2.0/data/cluster_uniref100/cluster_uniref100_q2-3_val.hash"

# used for offline training - point to the dir with version ending!
precomputed_dir: "/home/dtuteam/workspace/PLM_project_2.0/outputs/test/version_0" 
precomputed_batches_dir: "/home/dtuteam/workspace/PLM_project_2.0/data/cluster_uniref100/shred20_cluster_uniref100_q2-3"

# define run name and master output dir!
output_dir: "/home/dtuteam/workspace/PLM_project_2.0/outputs/"
run_name: "cloud_r4"

# hyper params
hyper_params:
  learning_rate: 0.001819

# training parameters
loop_params:
  student_model_param: "150M" # student model
  teacher_model_param: "3B" # teacher model
  save_masked_sequences: false # should tensor masking of seqeunces be saved
  log_batch_shape: false # extra per batch logging
  # should we use precomputed reps and logits?
  use_saved_reps_logs: false
  # save representations and logits per batch (only first epoch)
  save_reps_logs: false
  # save metrics per batch, by default its saved only per epoch
  save_per_batch: false

# sanity check for applied hashing
hashing_params:
  train_size_ratio: 0.7
  val_size_ratio: 0.2
  test_size_ratio: 0.1

# dataloader parameters
train_sampler_params:
  max_batch_tokens: 1024 # TESTED ON CLOUD DO NOT CHANGE
  drop_last: true
  shuffle: false # all samples before bucketing
  shuffle_batch_order: true # batch order after bucketing
  max_batch_num: -1  # max number of batches across all GPUs

# checkpoint parameters
checkpoint_params:
  every_n_epochs: 10 # save per epoch (use 'every_n_train_steps' for per batch)
  save_top_k: 1 # -1 if save all, or 1 to keep only the last one

# extra trainer settings
# enything here is passed to the trainer
trainer_params:
  max_epochs: 250 # set epoch number
